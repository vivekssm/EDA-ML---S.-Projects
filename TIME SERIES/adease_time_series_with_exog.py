# -*- coding: utf-8 -*-
"""AdEase_Time_Series_with_exog.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cTv7rTHLNhekn34HRUWaiUjhtzpSPIB7

# Ad Ease website Analytics

**Dataset:**
**Web Traffic Time Series Forecasting**

Forecasting the future values of multiple time series. More specifically the problem of forecasting future web traffic for approximately 145,000 wikipedia articles.

The training dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016.
For each time series, you are provided the name of the article as well as the type of traffic that this time series represent (all, mobile, desktop, spider). You may use this metadata and any other publicly available data to make predictions. Unfortunately, the data source for this dataset does not distinguish between traffic values of zero and missing values. A missing value may mean the traffic was zero or that the data is not available for that day.

***Data Dictionary:***

there are two csv files given


**train_1.csv : **

In the csv file, each row corresponds to a particular article and each column correspond to a particular date. The values are the number of visits in that date.


**Exog_Campaign_eng : **

this file contaings data for the dates which had a campaign or significant event that could affect the views for that day.
the data is just for pages in english.

there is a 1 for dates with campaign and 0 for remaining dates. It is to be treated as an exogenous variable for models when training and forecasting data for pages in english

## Intent of the notebook

1. We will start by loading the data and handling the values.

2. Then some Exploratory data analysis to get an understanding of the data and get some useful insight, based on various parameters, and visualizing them.

3. Preparing the data for feeding to the model(checking stationarity, transformations).

4. Preparing the model followed by some predictions.

5. Comparing the same with the given data and calculating the accuracy of the model.

**Importing the libraries**
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import pylab as p
import matplotlib.pyplot as plot
from collections import Counter
import re
import os
import seaborn as sns

import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter("ignore")

sns.set(rc={'figure.figsize':(11.7,8.27)})

#Data_link- https://drive.google.com/file/d/1sRlqw7-6S1u5p1YLoxJbyDZtmRegUooU/view?usp=sharing

train = pd.read_csv('/content/drive/MyDrive/train_1.csv')

"""Reading the dataset and printing head and tail to get basic idea"""

train.head()

print(train.info())

"""We can see that ther are some null values in the data, we will plot them to see how it looks"""

train.head(1000).isna().sum()

import seaborn as sbn
sbn.heatmap(train.head(1000).isna())

days = [r for r in range(1, len(train.columns))]
plot.figure(figsize=(10,7))
plot.xlabel('Day')
plot.ylabel('Null values')
plot.plot(days, train.isnull().sum()[1:])

"""We see that the number of nan values decrease with time.

Probable reason: Some website have all nan values in the begining, that can be due to the fact that those were created after that time so there is no traffic reading for that time
"""

print(train.shape)
train=train.dropna(how='all')
#‘all’ : If all values are NA, drop that row or column.
print(train.shape)

train=train.dropna(thresh=300)
print(train.shape)

"""1. We try droping the rows that have all values as nan, none in our case.

2. We then also drop rows that have nan more than 300 days, because the time series for that would not make much sense

3. We fill all the remaining values with zero assuming there was no traffic on the date that the values are nan for.

"""

train=train.fillna(0)
train.tail()

"""# ***EDA***

The page values are in this format


 **SPECIFIC NAME _ LANGUAGE.wikipedia.org _ ACCESS TYPE _ ACCESS ORIGIN**

having information about page name, the main domain, device type used to access the page, and also the request origin(spider or browser agent)
"""

#train['langgg']=train['Page'].apply(lambda x: x[x.find('wikipedia')-3:x.find('wikipedia')-1 ])

#Usage of Regex
def split_page(page):
    w = re.split('_|\.', page)
    print(w)
    return ' '.join(w[:-5]), w[-5], w[-2], w[-1]

split_page('2NE1_zh.wikipedia.org_all-access_spider')

def split_page(page):
  w = re.split('_|\.', page)
  return ' '.join(w[:-5]), w[-5], w[-2], w[-1]

li = list(train.Page.apply(lambda x: split_page(str(x))))
df = pd.DataFrame(li)
df.columns = ['Title', 'Language', 'Access_type','Access_origin']
df = pd.concat([train, df], axis = 1)

"""We split the page name and get that information joining it with a temporary database.
below we get some rows to see the structure of the data
"""

df.head()

# prompt: countplot for language column using valuecounts

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
sns.countplot(x='Language', data=df)
plt.title('Countplot for Language column')
plt.show()

"""This above is the comparision number of articles in each language

{'ja':'Japanese', 'de':'German',  'en' : 'English',  'no_lang':'Media_File',  'fr':'French',  'zh':'Chinese',  'ru':'Russian',  'es':'Spanish'}
"""

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
sns.countplot(x='Access_type', data=df)
plt.title('Countplot for Language column')
plt.show()

"""This comparision shows that usage from desktop and mobile is almost the same"""

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
sns.countplot(x='Access_origin', data=df)
plt.title('Countplot for Language column')
plt.show()

"""This shows that organic view is far more than that of spiders or bots

**Now we want to compare the views for different languages **
"""

#here we see that the languages are not treated properly as there are commons and www
df.groupby('Language').count()

df[df['Language']=='commons']

# Checking another way of fetching the language out of the string
def lang(Page):
    val = re.search('[a-z][a-z].wikipedia.org',Page)
    if val:
        #print(val)
        #print(val[0][0:2] )

        return val[0][0:2]

    return 'no_lang'

df['Language']=df['Page'].apply(lambda x: lang(str(x)))

df.groupby('Language').count() #now the count has increased. You can go back and get it sorted

df_language=df.groupby('Language').mean().transpose()
df_language

df_language.reset_index(inplace=True)
df_language.set_index('index', inplace=True)

df_language.plot(figsize=(12,7))
plot.ylabel('Views per Page')

"""Ploting the data shows that articles in english get the most number of views as compared to different languages, there are some spikes at different times in different laguages

Ploting just for english because we are going to use this for our furthur investigation and predictions
"""

df_language['en'].plot(figsize=(12,7))
plot.ylabel('Views per Page')

total_view=df_language.copy()

#################################################################################################################################

"""# Checking the stationarity

Dickey-Fuller test

**Here the null hypothesis is that the TS is non-stationary**:
The test results comprise of a Test Statistic and some Critical Values for difference confidence levels.
"""

from statsmodels.tsa.stattools import adfuller
def df_test(x):
    result=adfuller(x)
    print('ADF Stastistic: %f'%result[0])
    print('p-value: %f'%result[1])

df_test(total_view['en'])

"""We see that the p value is not low enough(<0.05). Therefore, we can say our series in not stationary as we fail to reject the null hypothesis

# Making the time series stationary
"""

ts=total_view['en']

"""## 1. Remove trend and seasonality with decomposition"""

# Naive decomposition of our Time Series as explained above
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(ts.values, model='multiplicative',extrapolate_trend='freq', period=7)

""" Additive or multiplicative?
  It’s important to understand what the difference between a multiplicative time series and an additive one before we go any further.

  There are three components to a time series:
  – trend how things are overall changing
  – seasonality how things change within a given period e.g. a year, month, week, day
  – error/residual/irregular activity not explained by the trend or the seasonal value

  How these three components interact determines the difference between a multiplicative and an additive time series.

  In a multiplicative time series, the components multiply together to make the time series. If you have an increasing trend, the amplitude of seasonal activity increases. Everything becomes more exaggerated. This is common when you’re looking at web traffic.

  In an additive time series, the components add together to make the time series. If you have an increasing trend, you still see roughly the same size peaks and troughs throughout the time series. This is often seen in indexed time series where the absolute value is growing but changes stay relative.


"""

trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

plot.figure(figsize=(10,7))
plot.subplot(411)
plot.title('Observed = Trend + Seasonality + Residuals')
plot.plot(ts.values,label='Observed')
plot.legend(loc='best')
plot.subplot(412)
plot.plot(trend, label='Trend')
plot.legend(loc='best')
plot.subplot(413)
plot.plot(seasonal,label='Seasonality')
plot.legend(loc='best')
plot.subplot(414)
plot.plot(residual, label='Residuals')
plot.legend(loc='best')
plot.tight_layout()
plot.show()

ts_decompose=pd.DataFrame(residual).fillna(0)[0].values
df_test(ts_decompose)

"""We can see that our series is now stationary, we can also try diffrencing to see what results we can get.

# 2. Remove trend and seasonality with differencing
"""

ts_diff = ts - ts.shift(1)
plot.plot(ts_diff.values)
plot.show()

ts_diff.dropna(inplace=True)
df_test(ts_diff)

"""Also the p value is 0. So we can say that our graph is now stationery.
Now we can apply the ARIMA model

**How do we choose p,d,q**

a thumb rule that for choosing the p,q values are when the lag goes below the significant level
- we use PACF for p, here we see that till lag 5 there are significat lines, if we want our model to be simpler we can start with a smaller number like 3/4
- we use ACF for q. here we can see that lag 4 is below significant level so we will use till lag 3


as for d we can see that at 1 diffencing the series becomes stationary so we choose d as 1

# Plot the autocorreltaion and partial auto correlation functions

Plotting the graphs and getting the p,q,d values for arima
"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
acf=plot_acf(ts_diff,lags=20)
pacf=plot_pacf(ts_diff,lags=20)

"""https://people.duke.edu/~rnau/411arim3.htm

# ARIMA MODEL

from the decomposition we can see that there is a weekly seasonality and still some spikes in the residual, that may be because of some external factors, which we can take into account by using them as our exogenous variable
"""

# Exog_data is a exogenous feature
# Here is the link : - https://drive.google.com/file/d/1rD4OpoCtV45qbFUzo3vk7a83747jTJCh/view?usp=sharing

ex_df = pd.read_csv('Exog_Campaign_eng')
ex_df.head()

"""We get the exogenous data from this csv file for english pages"""

exog=ex_df['Exog'].to_numpy()

"""we will train a sarimax model for that and see if we get anyimprovements from using the two information.

the seasonal order and the values of PDQ are based upon various trials and comparision of the models
- we see a seasonality of 7 from the plots ie: weekly seasonality ( from the plots we can see that afte some insignificant plots we have some significant values repeating at intervals of 7 ie: 7,14 ... )
- the non seasonal order we can keep the same
"""

import statsmodels.api as sm
train=ts[:520]
test=ts[520:]
model=sm.tsa.statespace.SARIMAX(train,order=(4, 1, 3),seasonal_order=(1,1,1,7),exog=exog[:520])
results=model.fit()

fc=results.forecast(30,dynamic=True,exog=pd.DataFrame(exog[520:]))

# Make as pandas series
fc_series = pd.Series(fc)
# Plot
train.index=train.index.astype('datetime64[ns]')
test.index=test.index.astype('datetime64[ns]')
plot.figure(figsize=(12,5), dpi=100)
plot.plot(train, label='training')
plot.plot(test, label='actual')
plot.plot(fc_series, label='forecast')

plot.title('Forecast vs Actuals')
plot.legend(loc='upper left', fontsize=8)

mape = np.mean(np.abs(fc - test.values)/np.abs(test.values))
rmse = np.mean((fc - test.values)**2)**.5
print("mape:",mape)
print("rsme:",rmse)

"""The mean absolute percentage error and the root mean squared error is low

# regression for a time series
"""

ts_df=ts.to_frame()
ts_df.head()

ts_df.reset_index(level=0, inplace=True)
ts_df['date']=pd.to_datetime(ts_df['index'])
ts_df.drop(['index'],axis=1,inplace=True)
ts_df.head()

ts_df['day_of_week']=ts_df['date'].dt.day_name()
ts_df.head()

ts_df=pd.get_dummies(ts_df, columns = ['day_of_week'])

ts_df.head()

ts_df['exog']=ex_df['Exog']
ts_df['rolling_mean']=ts_df['en'].rolling(7).mean()

ts_df=ts_df.dropna()
ts_df.head()

X=ts_df[['day_of_week_Friday',	'day_of_week_Monday',	'day_of_week_Saturday',	'day_of_week_Sunday',	'day_of_week_Thursday',	'day_of_week_Tuesday',	'day_of_week_Wednesday',	'exog',	'rolling_mean']].copy()
y=ts_df[['en']]

train_x = X[:-20]
test_x = X[-20:]

train_y = y[:-20]
test_y = y[-20:]

from sklearn.linear_model import LinearRegression

# Train and pred
model = LinearRegression()
model.fit(train_x, train_y)
y_pred = (model.predict(test_x))


mape = np.mean(np.abs(y_pred - test_y.values)/np.abs(test_y.values))
print("mape:",mape)

"""We can see here that aur mape is better than our arima model but worse than our sarimax model

# using Facebook Prophet
"""

!pip install prophet

ts_df['ds']=ts_df['date']
ts_df['y']=ts_df['en']

df2=ts_df[['date','en','exog']].copy()
df2.columns = ['ds', 'y', 'exog']
df2.head()

df2[:-20].info()

"""prophet without exogenous"""

from prophet import Prophet
m = Prophet(weekly_seasonality=True)
m.fit(df2[['ds', 'y']][:-20])
future = m.make_future_dataframe(periods=20,freq="D")
forecast = m.predict(future)
fig = m.plot(forecast)

"""prophet with exogenous"""

model2=Prophet(interval_width=0.9, weekly_seasonality=True, changepoint_prior_scale=1)
model2.add_regressor('exog')
model2.fit(df2[:-20])
forecast2 = model2.predict(df2)
fig = model2.plot(forecast2)

y_true = df2['y'].values
y_pred = forecast2['yhat'].values

plot.plot(y_true, label='Actual')
plot.plot(y_pred, label='Predicted')
plot.legend()
plot.show()

mape = np.mean(np.abs(forecast2['yhat'][-20:] - df2['y'][-20:].values)/np.abs(df2['y'][-20:].values))
print("mape:",mape)

"""# ------------------------------------------------------------------------------------------------------------------------------------------"""